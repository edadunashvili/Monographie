{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elguja Dadunashvili: Machine-Learning-Verfahren für die typologisch-vergleichenden Märchenforschung - Teil 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modul für die Aggregation des Korpus von unbekannten Daten "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 1]\n",
    "\n",
    "1-5: Der vorhandene Daten-Korpus löschen (falls vorhanden). Die Funktion wird im Folgenden zwecks Beseitigung von weiteren Aggregationen verwendet.\n",
    "\n",
    "6-61: Aggregation des Rohdatenkorpus.\n",
    "\n",
    "62-70: Aggregation der \"episode_string_such.csv\" Datei.\n",
    "\n",
    "71-76: 'episode_string_such.csv' laden, von den überflüssigen Zeichen bereinigen, fehlende Feldwerte auffüllen und in 'episode_binar_such.csv' umwandeln.\n",
    "\n",
    "77-78: Unnotige Aggregationen werden gelöscht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entfernen(var):\n",
    "    import os\n",
    "    if os.path.exists(var):\n",
    "        os.remove(var)\n",
    "entfernen('episode_binar_roh.csv')\n",
    "import glob\n",
    "def word_to_lex(word):\n",
    "    ret=(word) \n",
    "    return ret\n",
    "def write_back(words):\n",
    "    with open(\"episode_temp_roh.csv\", \"a\", encoding='utf-8') as output:      \n",
    "        for word in words:\n",
    "            as_lex = word_to_lex(word[0])\n",
    "            full_word = '\"' + as_lex + '\"'\n",
    "            for sub_word in word[1:]:\n",
    "                full_word += \" , \"  '\"' + sub_word + '\"'\n",
    "            full_word +=\"\\n\"\n",
    "            output.write(full_word)\n",
    "def clean(line):\n",
    "    line = line.replace(\"\\n\",\" \").strip()\n",
    "    line = line.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\")\n",
    "    line = line.replace(\"ß\",\"ss\").replace(\",\",\"\").replace(\"«\",\"\")\n",
    "    line = line.replace(\"»\",\"\").replace(\".\",\"\").replace(\":\",\"\")\n",
    "    line = line.replace(\";\",\"\").replace('\"',\"\")\n",
    "    line = line.replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\")\n",
    "    line = line.replace(\",\",\"\").replace(\"\\t\",\" \").replace(\"'\",\"\")\n",
    "    line = line.replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \")\n",
    "    line = line.replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','')\n",
    "    line = line.replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \")\n",
    "    line = line.replace('–','').replace('—','').replace('<','')\n",
    "    line = line.replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','')\n",
    "    line = line.replace('„','').replace('(','').replace(')','')\n",
    "    line = line.replace(\"‚\", \"\").replace(']','').replace('[','')\n",
    "    if line == \"\": \n",
    "        return\n",
    "    line=line.split(\"|\")\n",
    "    line[0]=line[0].split(\"|\")[0]\n",
    "    for i, _ in enumerate(line):\n",
    "        if (i !=0) and (i!=2):\n",
    "            line[i]=line[i].lower()\n",
    "    flex=[]\n",
    "    try:\n",
    "        flex=line[1].split(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    value=str(line)\n",
    "    line=str(line)   \n",
    "    flex.append(line)\n",
    "    ret=[]\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "with open(\"episode_temp_roh.csv\", \"w\", encoding='utf-8') as output:\n",
    "    output.write (\"quelle,episode,index_string,index_binar\\n\")\n",
    "pairs = []\n",
    "for file in glob.glob(\"Datendepot/*.txt\"):\n",
    "    with open(file, 'r', encoding='utf-8', errors='ignore') as episode:\n",
    "        for line in episode.readlines():\n",
    "            clean_words = clean(line)\n",
    "            pairs = pairs + clean_words\n",
    "write_back(pairs)\n",
    "from collections import Counter\n",
    "fin = open(\"episode_temp_roh.csv\",'r', encoding ='utf-8')\n",
    "fout = open('episode_string_roh.csv', \"wt\", encoding ='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(', \"[\"',\"\").replace('\"[', \"\")\n",
    "               .replace(']\"',\"\").replace(\"', '\", \"','\")\n",
    "               .replace(\" '\", \"'\"))   \n",
    "fin.close()\n",
    "fout.close()\n",
    "fin = open('episode_string_roh.csv','r', encoding='utf-8') \n",
    "fout = open('episode_binar_roh.csv', \"wt\", encoding='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(\"'\",\"\").replace(',,0,0',',test,0,0')) #dabei wird leergebliebene Datenfeld \"episode\" mit dem Wert 'test' aufgefüllt        \n",
    "fin.close()\n",
    "fout.close()\n",
    "entfernen('episode_temp_roh.csv')\n",
    "entfernen('episode_string_roh.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modul für die Suche nach den Trainingsdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 2]\n",
    "\n",
    "1-11: Rohdatenkopus aus dem Datenfeld 'epsode' bestimmen und alle andere Felder ignorieren.\n",
    "\n",
    "12-17: Datenfeld 'episode' von Stoppwörter bereinigen, zur Auffüllung der fehlende Werte eingesetzte synthetischer Stoff (hier das Wort 'test') entfernen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('episode_binar_roh.csv', encoding='utf-8')\n",
    "t_corpus = []\n",
    "file = 'episode_binar_roh.csv'\n",
    "with open(file, \"r\", encoding='utf-8') as maerchen:\n",
    "    maerchen = df.episode\n",
    "    reader = csv.reader(maerchen, delimiter = ',')\n",
    "    for row in reader:\n",
    "        lst = str(row)\n",
    "        t_corpus.append(lst)\n",
    "custom_stop_word_list=['test']\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "NLTK_stop_words_list=stopwords.words('german')\n",
    "de = custom_stop_word_list + NLTK_stop_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 3]\n",
    "\n",
    "1-14: Vektorisieren, Transformieren und die Metrik für die Vorhersagefunktion definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vectorizer = TfidfVectorizer\n",
    "vectorizers = vectorizer(analyzer='word', ngram_range=(1,1), \n",
    "                         min_df = 2, stop_words = de)\n",
    "vokabular =  vectorizers.fit_transform(t_corpus)\n",
    "metrik = cosine_similarity\n",
    "def find_similar(vokabular, index, top_n =-1):   \n",
    "    metriks = metrik(vokabular[index:index+1], vokabular).flatten()\n",
    "    related_docs_indices = [i for i in metriks.argsort()[::-1] \n",
    "                            if i != index]\n",
    "    return [(index, metriks[index]) for index \n",
    "            in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nach den inhaltlich ähnlichen Datensätzen suchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 4]\n",
    "\n",
    "1-3: Gesuchte Episode (Zielepisode) eingeben und einblenden lassen.\n",
    "\n",
    "Anmerkung: Die gesuchte Episode wird über die 'n_te'-Variable eingegeben (manuell eingepflegte Zielepisoden sollen deshalb immer in der ersten Reihen der Korpusdatensätzen stehen und alphanummerisch angeordnet sein. Wir bevorzugen, dass diese Daten mit dem Prefix '!_' + Kleinbuchstabe benannt wird.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_te = 0\n",
    "synt = t_corpus[n_te]\n",
    "print(df.quelle[n_te],'-', synt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 5]\n",
    "\n",
    "1-3: Suchen starten und die Ergebnisse einblenden lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, score in (find_similar(vokabular, n_te)):\n",
    "        print(index,'-', score,'-', df.quelle[index],'-', \n",
    "           t_corpus[index],'-', df.index_string[index],'-', df.index_binar[index],'\\n' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ende "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
