{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Künstlicher Assistent zur Annotation von Märchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab Arbeitplatz vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/edadunashvili/Monographie.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/Monographie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade joblib==1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation des Trainingsdatenkorpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 1]\n",
    "\n",
    "1-3: Die Variablen 'typNum', 'repert' und 'ausnahme' gelten für die Auswahl der im Trainingsdatenrepositorium vorhandenen Texte mit dem gesuchten Typ und dem Herkunftsrepertoire. Sowohl die Variable 'typNum' als auch 'repert' können mit leeren Anführungszeichen dargestellt werden. Für die Variable 'ausnahme' ist entweder ein unwahrscheinlicher Wert (z.B. 'xxyyzz') oder der Wert, mit dem eine bestimmte Gruppe von Daten von der Auswahl ausgeschlossen werden kann, verpflichtend.\n",
    "\n",
    "4-8: Bei jeder Aggregation des Trainingsdatenkorpus wird der gleichnamige vorherige Korpus gelöscht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typNum='300'\n",
    "repert=''\n",
    "ausnahme='xxyyzz'\n",
    "def entfernen(var):\n",
    "    import os\n",
    "    if os.path.exists(var):\n",
    "        os.remove(var)\n",
    "entfernen('episode_binar_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 2]\n",
    "\n",
    "1-59: Aggregation der Datei 'episode_temp_train.csv' (eine Funktion). Die im Ordner „Trainingsdaten“ angelegten TXT-Dateien werden in der CSV-Datei aggregiert und in das Root-Repository verschoben.\n",
    "\n",
    "60-66: Umwandlung von 'episode_temp_train.csv' in die Datei 'episode_string_train.csv'.\n",
    "\n",
    "67-72: Laden von 'episode_string_train.csv', Bereinigung von überflüssigen Zeichen und Umwandlung in das Trainingsdatenkorpus 'episode_binar_train.csv'.\n",
    "\n",
    "ab 73: Unnötige Aggregationen werden gelöscht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(line):\n",
    "    line = line.replace(\"\\n\",\" \").strip()\n",
    "    line = line.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\")\n",
    "    line = line.replace(\"ß\",\"ss\").replace(\",\",\"\").replace(\"«\",\"\")\n",
    "    line = line.replace(\"»\",\"\").replace(\".\",\"\").replace(\":\",\"\")\n",
    "    line = line.replace(\";\",\"\").replace('\"',\"\")\n",
    "    line = line.replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\")\n",
    "    line = line.replace(\",\",\"\").replace(\"\\t\",\" \").replace(\"'\",\"\")\n",
    "    line = line.replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \")\n",
    "    line = line.replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','')\n",
    "    line = line.replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \")\n",
    "    line = line.replace('–','').replace('—','').replace('<','')\n",
    "    line = line.replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','')\n",
    "    line = line.replace('„','').replace('(','').replace(')','')\n",
    "    line = line.replace(\"‚\", \"\").replace(']','').replace('[','')\n",
    "    if line == \"\": \n",
    "        return\n",
    "    line=line.split(\"|\")\n",
    "    line[0]=line[0].split(\"|\")[0]\n",
    "    for i, _ in enumerate(line):\n",
    "        if (i !=0) and (i!=2):\n",
    "            line[i]=line[i].lower()\n",
    "    flex=[]\n",
    "    try:\n",
    "        flex=line[1].split(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    value=str(line)\n",
    "    line=str(line)   \n",
    "    flex.append(line)\n",
    "    ret=[]\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "def write_back(words, temp):\n",
    "    with open(temp, \"a\", encoding='utf-8') as output:      \n",
    "        for word in words:\n",
    "            as_lex = word[0]\n",
    "            full_word = '\"' + as_lex + '\"'\n",
    "            for sub_word in word[1:]:\n",
    "                full_word += \" , \"  '\"' + sub_word + '\"'\n",
    "            full_word +=\"\\n\"\n",
    "            output.write(full_word)\n",
    "def aggreg(temp, ordner):\n",
    "    import glob\n",
    "    with open(temp, \"w\", encoding='utf-8') as output:\n",
    "        output.write (\"quelle,episode,index_string,index_binar\\n\")\n",
    "    sorted_files=sorted(glob.glob(ordner))\n",
    "    pairs = []\n",
    "    for file in sorted_files:\n",
    "        if typNum in file and repert in file and not ausnahme in file:\n",
    "            with open(file, 'r', encoding='utf-8', errors='ignore') as episode:\n",
    "                for line in episode.readlines():\n",
    "                    clean_words = clean(line)\n",
    "                    pairs = pairs + clean_words\n",
    "    write_back(pairs, temp)\n",
    "temp = \"episode_temp_train.csv\"\n",
    "ordner = \"Trainingsdaten/*.txt\"\n",
    "aggreg(temp, ordner)\n",
    "fin = open('episode_temp_train.csv','r', encoding ='utf-8')\n",
    "fout = open('episode_string_train.csv', \"wt\", encoding ='utf-8')\n",
    "for kfz in fin:\n",
    "    fout.write(kfz.replace(', \"[\"',\"\").replace('\"[', \"\").replace(']\"',\"\")\n",
    "               .replace(\"', '\", \"','\").replace(\" '\", \"'\"))\n",
    "fin.close()\n",
    "fout.close()\n",
    "fin = open('episode_string_train.csv','r', encoding='utf-8') \n",
    "fout = open('episode_binar_train.csv', \"wt\", encoding='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(\"'\",\"\"))             \n",
    "fin.close()\n",
    "fout.close()\n",
    "entfernen('episode_temp_train.csv')\n",
    "entfernen('episode_string_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Herstellung des Modells und der prototypischen Metaepisode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 3]\n",
    "\n",
    "1-5: Zugriff auf den Trainingsdatenkorpus nehmen und die im Datenfeld 'index_string' verzeichneten Merkmale sowie deren Häufigkeit anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "df = pd.read_csv('episode_binar_train.csv',  encoding='utf-8')\n",
    "indexliste=Counter(df.index_string)\n",
    "print(indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 4]\n",
    "\n",
    "1: Eeine Episode Kopieren und sie als Wert der Variable 'gesep' hier einfügen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesep = 'e300_c_anfangssituation_ankunft_und_erkundigung_der_not'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 5]\n",
    "\n",
    "1-6: In den Trainingsdaten die gesuchte Episode ('gesep') mit Einsen (1) und den Rest mit Nullen (0) kennzeichnen, und die Änderungen speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gesuchte_episode in enumerate(df.index_string):\n",
    "    if (gesuchte_episode == gesep):\n",
    "        df.index_binar[i]='1'\n",
    "    else: \n",
    "            df.index_binar[i]='0'\n",
    "df.to_csv('episode_binar_train.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 6]\n",
    "\n",
    "1-4: Im Trainingsdatenkorpus die Datenfelder für Probetexte (X_train) und für die Merkmale (y_train) bestimmen und das Gleichgewicht zwischen negativ (0) und positiv (1) etikettierten Datensätzen analysieren (siehe Counter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=(df['episode'])\n",
    "y_train=(df['index_binar'])\n",
    "indexliste=Counter(df.index_binar)\n",
    "print(indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 7]\n",
    "\n",
    "1: Die in die eckigen Klammern eingetragenen Wörter werden zusätzlich in die vorhandene Stopwörter-Liste eingefügt. Einzelne Wörter müssen dabei mit Anführungszeichen umschlossen und von anderen Wörtern durch Kommas getrennt werden, wie zum Beispiel: ['mein', 'dein'].\n",
    "\n",
    "2-6: Das Datenfeld 'episode' von konventionellen und von uns eingetragenen Stoppwörtern bereinigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_word_list=[]\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "NLTK_stop_words_list=stopwords.words('german')\n",
    "de = custom_stop_word_list + NLTK_stop_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 8]\n",
    "\n",
    "1-8: Einrichtung einer sciki-learn Pipeline mit Klassen LogisticRegression, TfidVectorizer und GridSearchCV. \n",
    "\n",
    "9-10: Modell auswerten (s. Bester Score aus der Kreuzvalidierung:)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=3, stop_words=de, ngram_range=(1,1), norm=None), \n",
    "                     LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}     \n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Bester Score aus der Kreuzvalidierung: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 9]\n",
    "\n",
    "1-6: Aus dem Modell die Liste der positiven und negativen Merkmale extrahieren, die Anzahl der Merkmale anzeigen (siehe Gesamte Anzahl der Wörter:)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "max_value = vectorizer.transform(X_train).max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "print(\"Gesamte Anzahl der Wörter: {}\".format(len(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 10]\n",
    "\n",
    "1-6: Merkmale mit den höchsten Koeffizienten visualisieren (blau=positiv, rot=negativ). Die Variable 's_zahl' legt fest, wie viele Merkmale beider Kategorien eingeblendet werden sollen.\n",
    "\n",
    "7: Gibt den genauen Wert des kleinsten und höchsten Koeffizienten zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_zahl = 200\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "mglearn.tools.visualize_coefficients(\n",
    "    grid.best_estimator_.named_steps[\"logisticregression\"].coef_, \n",
    "    feature_names, n_top_features=s_zahl)\n",
    "plt.grid()\n",
    "#plt.loglog()\n",
    "plt.ylim()\n",
    "#plt.savefig(gesep+'.jpg', dpi=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 11a]\n",
    "\n",
    "1-7: Extrahieren der Top-Klassifikationsmerkmale mit positiven und negativen Werten und ihrer Koeffizienten.\n",
    "\n",
    "8-11: Visualisierung der ausgewählten Merkmale bzw. Koeffizienten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gio=(grid.best_estimator_.named_steps[\"logisticregression\"].coef_[0])\n",
    "n_top=np.argsort(gio)\n",
    "z_top=np.sort(gio)\n",
    "text_pos=feature_names[n_top[-s_zahl:]]\n",
    "text_neg=feature_names[n_top[0:s_zahl]]\n",
    "zahl_pos=gio[n_top[-s_zahl:]]\n",
    "zahl_neg=gio[n_top[0:s_zahl]]\n",
    "#print(\"Positive Merkmale:\\n{}\".format(text_pos))\n",
    "#print (\"Negative Merkmale:\\n{}\".format(text_neg))\n",
    "#print (\"Positive coef:\\n{}\".format(zahl_pos))\n",
    "#print (\"Negative coef:\\n{}\".format(zahl_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Zelle # 11b]\n",
    " \n",
    "1-7: Berechnung des Schwellenwerts nach der Gleichung (y, x) + (-y, -x). Wenn die Gleichung den Wert 0 ergibt, ist der Schwellenwert gleich x.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfz=range(s_zahl)\n",
    "folgenum=0\n",
    "for x in kfz:\n",
    "    folgenum+=1\n",
    "    vollliste=(zahl_pos[s_zahl-folgenum])+(zahl_neg[s_zahl-folgenum])\n",
    "    rundliste=(round(vollliste,3))\n",
    "    print(rundliste, '-', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 11c]\n",
    "\n",
    "Schwellenwert für die Variable 'sw' manuel eintragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 12]\n",
    "\n",
    "1-17: Bestimmte Merkmale in die neue txt Datei speichern(1-7); Gesamte Information durch die Datenfelder strukturieren (8-12) und nach dem Löschen der alten Datei (13) neue in Messdatenordner anlegen (14-17)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import shutil, os\n",
    "filename = '0_'+gesep\n",
    "tit=(text_pos[s_zahl-sw:s_zahl])\n",
    "with open (filename+'.txt', 'wt', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=' ')\n",
    "    writer.writerow(tit)\n",
    "with open(filename+'.txt', 'r') as file:\n",
    "    file_contents = file.read().replace(\"\\n\", \"\")\n",
    "    frt_contents = filename+'|'+file_contents+'|0|0'\n",
    "with open(filename+'.txt', 'wt', encoding='utf-8') as f:\n",
    "        f.write(frt_contents)\n",
    "entfernen('Messdaten/'+filename+'.txt')\n",
    "import glob\n",
    "files = glob.glob(filename+'.txt')\n",
    "for f in files:\n",
    "    shutil.move(f, 'Messdaten/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Herstellung des Messdatenkorpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 13]\n",
    "\n",
    "1-3: Die Variablen 'typNum', 'repert' und 'ausnahme' gelten für die Auswahl der im Trainingsdatenrepositorium vorhandenen Texte mit dem gesuchten Typ und dem Herkunftsrepertoire. Sowohl die Variable 'typNum' als auch 'repert' können mit leeren Anführungszeichen dargestellt werden. Für die Variable 'ausnahme' ist entweder ein unwahrscheinlicher Wert (z.B. 'xxyyzz') oder der Wert, mit dem eine bestimmte Gruppe von Daten von der Auswahl ausgeschlossen werden kann, verpflichtend.\n",
    "\n",
    "4: Den vorhandenen Messdatenkorpus löschen (falls vorhanden).\n",
    "\n",
    "5-7: Im Messdatenordner die Messdaten parsen und einen Korpus 'episode_temp_mess.csv' aggregieren.\n",
    "\n",
    "8-16: 'episode_temp_mess.csv' von überflüssigen Zeichen bereinigen und in den Messdatenkorpus 'episode_binar_mess.csv' umwandeln.\n",
    "\n",
    "17-21: Datenfelder deklarieren, Datensätze zählen (siehe Counter), anschließend unnötige temporäre Aggregationen löschen.\n",
    "\n",
    "22-29: Den Messdatenkorpus aus den Episoden bestimmen und alle anderen Datenfelder ignorieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "typNum=''\n",
    "repert=''\n",
    "ausnahme='xxyyzz'\n",
    "entfernen('episode_binar_mess.csv')\n",
    "temp = \"episode_temp_mess.csv\"\n",
    "ordner = \"Messdaten/*.txt\"\n",
    "aggreg(temp, ordner)\n",
    "from collections import Counter\n",
    "fin = open(\"episode_temp_mess.csv\",'r', encoding ='utf-8')\n",
    "fout = open('episode_binar_mess.csv', \"wt\", encoding ='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(', \"[\"',\"\").replace('\"[', \"\")\n",
    "               .replace(']\"',\"\").replace(\"', '\", \"','\")\n",
    "               .replace(\" '\", \"'\").replace(\"'\",\"\"))   \n",
    "fin.close()\n",
    "fout.close()\n",
    "df = pd.read_csv('episode_binar_mess.csv', encoding='utf-8')\n",
    "from collections import Counter\n",
    "indexliste=Counter(df.index_binar)\n",
    "print(indexliste)\n",
    "entfernen('episode_temp_mess.csv')\n",
    "t_corpus = []\n",
    "file = 'episode_binar_mess.csv'\n",
    "with open(file, \"r\", encoding='utf-8') as maerchen:\n",
    "    maerchen = df.episode\n",
    "    reader = csv.reader(maerchen, delimiter = \"|\") \n",
    "    for row in reader:       \n",
    "        lst = str(row)\n",
    "        t_corpus.append(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorhersage von Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 14]\n",
    "\n",
    "1-2: Vorhersagen treffen.\n",
    "\n",
    "3: Der synthetische Datensatz wird ausgeschlossen.\n",
    "\n",
    "4: Jeder positiv bewertete Datensatz wird separat eingeblendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_pred=grid.predict(t_corpus)\n",
    "for index in enumerate(mod_pred): \n",
    "    if '1' in (index) and index[0]>0:   \n",
    "        print ('Positiv bewerteter Datensatz:{}'.format(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 15]\n",
    "\n",
    "1-7: Alle probabilistischen Vorhersagen abrufen. Durch die Anpassung der Variable 'pos>=' in Zeile 6 kann der minimale Score manipuliert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_pred=grid.predict_proba(t_corpus) \n",
    "sort_index=np.flip(all_pred[:,1].argsort())\n",
    "for num_pos in sort_index:\n",
    "    pos=all_pred[num_pos][1]\n",
    "    for i,t_corpus_element in enumerate(t_corpus):\n",
    "        if i==num_pos  and num_pos>0 and pos>0.007:\n",
    "            print(num_pos,'-', pos,'-', df.quelle[num_pos],'-', df.episode[num_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Vorhersage von Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 16]\n",
    "\n",
    "1-14: Daten im Messdatenkorpus Vektorisieren, Transformieren und die Metrik für die Vorhersagefunktion definieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vectorizer = TfidfVectorizer\n",
    "vectorizers = vectorizer(analyzer='word', ngram_range=(1,1),\n",
    "                         min_df = 2, stop_words = de)\n",
    "vokabular =  vectorizers.fit_transform (t_corpus)\n",
    "metrik = cosine_similarity\n",
    "def find_similar(vokabular, index, top_n =-1):   \n",
    "    metriks = metrik(vokabular[index:index+1], vokabular).flatten()\n",
    "    related_docs_indices = [i for i in metriks.argsort()[::-1] \n",
    "                            if i != index]\n",
    "    return [(index, metriks[index]) for index \n",
    "            in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 17]\n",
    "\n",
    "1-3: Gesuchte Episode (Zielepisode) einblenden. Die gesuchte Episode wird durch die 'n_te'-Variable bestimmt. Manuell eingepflegte Zielepisoden sollten deshalb immer in der ersten Reihe der Korpusdatensätze stehen. Wir empfehlen daher, diese Daten mit dem Präfix '0_' zu benennen.\n",
    "\n",
    "4: Synthetische Datei aus dem Messdatenordner entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_te = 0\n",
    "synt = t_corpus[n_te]\n",
    "print(df.quelle[n_te],'-', synt)\n",
    "entfernen('Messdaten/'+'0_'+gesep+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 18]\n",
    "\n",
    "1-3: Suchen starten und die Ergebnisse einblenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, score in (find_similar(vokabular, n_te)):\n",
    "    if (df.index_string[index]==0 and score>0.007):\n",
    "        print(index,'-', score,'-', df.quelle[index],'-', \n",
    "           t_corpus[index], '-',  df.index_binar[index],'-', df.index_binar[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
