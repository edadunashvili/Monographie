{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Künstlicher Assistent zur Annotation von Märchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab Arbeitplatz vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/edadunashvili/Monographie.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/Monographie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn==1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade joblib==1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation des Trainingsdatenkorpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 1]\n",
    "\n",
    "1-3: Die Variablen 'typNum', 'repert' und 'ausnahme' gelten für die Auswahl der im Trainingsdatenrepositorium vorhandenen Texte mit dem gesuchten Typ und dem Herkunftsrepertoire. Sowohl die Variable 'typNum' als auch 'repert' können mit leeren Anführungszeichen dargestellt werden. Für die Variable 'ausnahme' ist entweder ein unwahrscheinlicher Wert (z.B. 'xxyyzz') oder der Wert, mit dem eine bestimmte Gruppe von Daten von der Auswahl ausgeschlossen werden kann, verpflichtend.\n",
    "\n",
    "4-8: Bei jeder Aggregation des Trainingsdatenkorpus wird der gleichnamige vorherige Korpus gelöscht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typNum='300'\n",
    "repert=''\n",
    "ausnahme='xxyyzz'\n",
    "def entfernen(var):\n",
    "    import os\n",
    "    if os.path.exists(var):\n",
    "        os.remove(var)\n",
    "entfernen('episode_binar_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 2]\n",
    "\n",
    "1-59: Aggregation der Datei 'episode_temp_train.csv' (eine Funktion). Die im Ordner „Trainingsdaten“ angelegten TXT-Dateien werden in der CSV-Datei aggregiert und in das Root-Repository verschoben.\n",
    "\n",
    "60-66: Umwandlung von 'episode_temp_train.csv' in die Datei 'episode_string_train.csv'.\n",
    "\n",
    "67-72: Laden von 'episode_string_train.csv', Bereinigung von überflüssigen Zeichen und Umwandlung in das Trainingsdatenkorpus 'episode_binar_train.csv'.\n",
    "\n",
    "ab 73: Unnötige Aggregationen werden gelöscht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(line):\n",
    "    line = line.replace(\"\\n\",\" \").strip()\n",
    "    line = line.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\")\n",
    "    line = line.replace(\"ß\",\"ss\").replace(\",\",\"\").replace(\"«\",\"\")\n",
    "    line = line.replace(\"»\",\"\").replace(\".\",\"\").replace(\":\",\"\")\n",
    "    line = line.replace(\";\",\"\").replace('\"',\"\")\n",
    "    line = line.replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\")\n",
    "    line = line.replace(\",\",\"\").replace(\"\\t\",\" \").replace(\"'\",\"\")\n",
    "    line = line.replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \")\n",
    "    line = line.replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','')\n",
    "    line = line.replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \")\n",
    "    line = line.replace('–','').replace('—','').replace('<','')\n",
    "    line = line.replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','')\n",
    "    line = line.replace('„','').replace('(','').replace(')','')\n",
    "    line = line.replace(\"‚\", \"\").replace(']','').replace('[','')\n",
    "    if line == \"\": \n",
    "        return\n",
    "    line=line.split(\"|\")\n",
    "    line[0]=line[0].split(\"|\")[0]\n",
    "    for i, _ in enumerate(line):\n",
    "        if (i !=0) and (i!=2):\n",
    "            line[i]=line[i].lower()\n",
    "    flex=[]\n",
    "    try:\n",
    "        flex=line[1].split(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    value=str(line)\n",
    "    line=str(line)   \n",
    "    flex.append(line)\n",
    "    ret=[]\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "def write_back(words, temp):\n",
    "    with open(temp, \"a\", encoding='utf-8') as output:      \n",
    "        for word in words:\n",
    "            as_lex = word[0]\n",
    "            full_word = '\"' + as_lex + '\"'\n",
    "            for sub_word in word[1:]:\n",
    "                full_word += \" , \"  '\"' + sub_word + '\"'\n",
    "            full_word +=\"\\n\"\n",
    "            output.write(full_word)\n",
    "def aggreg(temp, ordner):\n",
    "    import glob\n",
    "    with open(temp, \"w\", encoding='utf-8') as output:\n",
    "        output.write (\"quelle,episode,index_string,index_binar\\n\")\n",
    "    sorted_files=sorted(glob.glob(ordner))\n",
    "    pairs = []\n",
    "    for file in sorted_files:\n",
    "        if typNum in file and repert in file and not ausnahme in file:\n",
    "            with open(file, 'r', encoding='utf-8', errors='ignore') as episode:\n",
    "                for line in episode.readlines():\n",
    "                    clean_words = clean(line)\n",
    "                    pairs = pairs + clean_words\n",
    "    write_back(pairs, temp)\n",
    "temp = \"episode_temp_train.csv\"\n",
    "ordner = \"Trainingsdaten/*.txt\"\n",
    "aggreg(temp, ordner)\n",
    "fin = open('episode_temp_train.csv','r', encoding ='utf-8')\n",
    "fout = open('episode_string_train.csv', \"wt\", encoding ='utf-8')\n",
    "for kfz in fin:\n",
    "    fout.write(kfz.replace(', \"[\"',\"\").replace('\"[', \"\").replace(']\"',\"\")\n",
    "               .replace(\"', '\", \"','\").replace(\" '\", \"'\"))\n",
    "fin.close()\n",
    "fout.close()\n",
    "fin = open('episode_string_train.csv','r', encoding='utf-8') \n",
    "fout = open('episode_binar_train.csv', \"wt\", encoding='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(\"'\",\"\"))             \n",
    "fin.close()\n",
    "fout.close()\n",
    "entfernen('episode_temp_train.csv')\n",
    "entfernen('episode_string_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Herstellung des Modells und der prototypischen Metaepisode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 3]\n",
    "\n",
    "1-5: Zugriff auf den Trainingsdatenkorpus nehmen und die im Datenfeld 'index_string' verzeichneten Merkmale sowie deren Häufigkeit anzeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "df = pd.read_csv('episode_binar_train.csv',  encoding='utf-8')\n",
    "indexliste=Counter(df.index_string)\n",
    "print(indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 4]\n",
    "\n",
    "1: Eeine Episode Kopieren und sie als Wert der Variable 'gesep' hier einfügen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesep = 'e300_c_anfangssituation_ankunft_und_erkundigung_der_not'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 5]\n",
    "\n",
    "1-6: In den Trainingsdaten die gesuchte Episode ('gesep') mit Einsen (1) und den Rest mit Nullen (0) kennzeichnen, und die Änderungen speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gesuchte_episode in enumerate(df.index_string):\n",
    "    if (gesuchte_episode == gesep):\n",
    "        df.index_binar[i]='1'\n",
    "    else: \n",
    "            df.index_binar[i]='0'\n",
    "df.to_csv('episode_binar_train.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 6]\n",
    "\n",
    "1-4: Im Trainingsdatenkorpus die Datenfelder für Probetexte (X_train) und für die Merkmale (y_train) bestimmen und das Gleichgewicht zwischen negativ (0) und positiv (1) etikettierten Datensätzen analysieren (siehe Counter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=(df['episode'])\n",
    "y_train=(df['index_binar'])\n",
    "indexliste=Counter(df.index_binar)\n",
    "print(indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 7]\n",
    "\n",
    "1: Die in die eckigen Klammern eingetragenen Wörter werden zusätzlich in die vorhandene Stopwörter-Liste eingefügt. Einzelne Wörter müssen dabei mit Anführungszeichen umschlossen und von anderen Wörtern durch Kommas getrennt werden, wie zum Beispiel: ['mein', 'dein'].\n",
    "\n",
    "2-6: Das Datenfeld 'episode' von konventionellen und von uns eingetragenen Stoppwörtern bereinigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_word_list=[]\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "NLTK_stop_words_list=stopwords.words('german')\n",
    "de = custom_stop_word_list + NLTK_stop_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 8]\n",
    "\n",
    "1-8: Einrichtung einer sciki-learn Pipeline mit Klassen LogisticRegression, TfidVectorizer und GridSearchCV. \n",
    "\n",
    "9-10: Modell auswerten (s. Bester Score aus der Kreuzvalidierung:)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(min_df=3, stop_words=de, ngram_range=(1,1), norm=None), \n",
    "                     LogisticRegression())\n",
    "param_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}     \n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Bester Score aus der Kreuzvalidierung: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 9]\n",
    "\n",
    "1-6: Aus dem Modell die Liste der positiven und negativen Merkmale extrahieren, die Anzahl der Merkmale anzeigen (siehe Gesamte Anzahl der Wörter:)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "max_value = vectorizer.transform(X_train).max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "print(\"Gesamte Anzahl der Wörter: {}\".format(len(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 10]\n",
    "\n",
    "1-6: Merkmale mit den höchsten Koeffizienten visualisieren (blau=positiv, rot=negativ). Die Variable 's_zahl' legt fest, wie viele Merkmale beider Kategorien eingeblendet werden sollen.\n",
    "\n",
    "7: Gibt den genauen Wert des kleinsten und höchsten Koeffizienten zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_zahl = 200\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "mglearn.tools.visualize_coefficients(\n",
    "    grid.best_estimator_.named_steps[\"logisticregression\"].coef_, \n",
    "    feature_names, n_top_features=s_zahl)\n",
    "plt.grid()\n",
    "#plt.loglog()\n",
    "plt.ylim()\n",
    "#plt.savefig(gesep+'.jpg', dpi=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 11a]\n",
    "\n",
    "1-7: Extrahieren der Top-Klassifikationsmerkmale mit positiven und negativen Werten und ihrer Koeffizienten.\n",
    "\n",
    "8-11: Visualisierung der ausgewählten Merkmale bzw. Koeffizienten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gio=(grid.best_estimator_.named_steps[\"logisticregression\"].coef_[0])\n",
    "n_top=np.argsort(gio)\n",
    "z_top=np.sort(gio)\n",
    "text_pos=feature_names[n_top[-s_zahl:]]\n",
    "text_neg=feature_names[n_top[0:s_zahl]]\n",
    "zahl_pos=gio[n_top[-s_zahl:]]\n",
    "zahl_neg=gio[n_top[0:s_zahl]]\n",
    "#print(\"Positive Merkmale:\\n{}\".format(text_pos))\n",
    "#print (\"Negative Merkmale:\\n{}\".format(text_neg))\n",
    "#print (\"Positive coef:\\n{}\".format(zahl_pos))\n",
    "#print (\"Negative coef:\\n{}\".format(zahl_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Zelle # 11b]\n",
    " \n",
    "1-7: Berechnung des Schwellenwerts nach der Gleichung (y, x) + (-y, -x). Wenn die Gleichung den Wert 0 ergibt, ist der Schwellenwert gleich x.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfz=range(s_zahl)\n",
    "folgenum=0\n",
    "for x in kfz:\n",
    "    folgenum+=1\n",
    "    vollliste=(zahl_pos[s_zahl-folgenum])+(zahl_neg[s_zahl-folgenum])\n",
    "    rundliste=(round(vollliste,3))\n",
    "    print(rundliste, '-', x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 11c]\n",
    "\n",
    "Schwellenwert für die Variable 'sw' manuel eintragen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw=123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 12]\n",
    "\n",
    "1-17: Bestimmte Merkmale in die neue txt Datei speichern(1-7); Gesamte Information durch die Datenfelder strukturieren (8-12) und nach dem Löschen der alten Datei (13) neue in Messdatenordner anlegen (14-17)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import shutil, os\n",
    "filename = '0_'+gesep\n",
    "tit=(text_pos[s_zahl-sw:s_zahl])\n",
    "with open (filename+'.txt', 'wt', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=' ')\n",
    "    writer.writerow(tit)\n",
    "with open(filename+'.txt', 'r') as file:\n",
    "    file_contents = file.read().replace(\"\\n\", \"\")\n",
    "    frt_contents = filename+'|'+file_contents+'|0|0'\n",
    "with open(filename+'.txt', 'wt', encoding='utf-8') as f:\n",
    "        f.write(frt_contents)\n",
    "entfernen('Messdaten/'+filename+'.txt')\n",
    "import glob\n",
    "files = glob.glob(filename+'.txt')\n",
    "for f in files:\n",
    "    shutil.move(f, 'Messdaten/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Herstellung des Messdatenkorpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 13]\n",
    "\n",
    "1-3: Die Variablen 'typNum', 'repert' und 'ausnahme' gelten für die Auswahl der im Trainingsdatenrepositorium vorhandenen Texte mit dem gesuchten Typ und dem Herkunftsrepertoire. Sowohl die Variable 'typNum' als auch 'repert' können mit leeren Anführungszeichen dargestellt werden. Für die Variable 'ausnahme' ist entweder ein unwahrscheinlicher Wert (z.B. 'xxyyzz') oder der Wert, mit dem eine bestimmte Gruppe von Daten von der Auswahl ausgeschlossen werden kann, verpflichtend.\n",
    "\n",
    "4: Den vorhandenen Messdatenkorpus löschen (falls vorhanden).\n",
    "\n",
    "5-7: Im Messdatenordner die Messdaten parsen und einen Korpus 'episode_temp_mess.csv' aggregieren.\n",
    "\n",
    "8-16: 'episode_temp_mess.csv' von überflüssigen Zeichen bereinigen und in den Messdatenkorpus 'episode_binar_mess.csv' umwandeln.\n",
    "\n",
    "17-21: Datenfelder deklarieren, Datensätze zählen (siehe Counter), anschließend unnötige temporäre Aggregationen löschen.\n",
    "\n",
    "22-29: Den Messdatenkorpus aus den Episoden bestimmen und alle anderen Datenfelder ignorieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "typNum=''\n",
    "repert=''\n",
    "ausnahme='xxyyzz'\n",
    "entfernen('episode_binar_mess.csv')\n",
    "temp = \"episode_temp_mess.csv\"\n",
    "ordner = \"Messdaten/*.txt\"\n",
    "aggreg(temp, ordner)\n",
    "from collections import Counter\n",
    "fin = open(\"episode_temp_mess.csv\",'r', encoding ='utf-8')\n",
    "fout = open('episode_binar_mess.csv', \"wt\", encoding ='utf-8')\n",
    "for efz in fin:\n",
    "    fout.write(efz.replace(', \"[\"',\"\").replace('\"[', \"\")\n",
    "               .replace(']\"',\"\").replace(\"', '\", \"','\")\n",
    "               .replace(\" '\", \"'\").replace(\"'\",\"\"))   \n",
    "fin.close()\n",
    "fout.close()\n",
    "df = pd.read_csv('episode_binar_mess.csv', encoding='utf-8')\n",
    "from collections import Counter\n",
    "indexliste=Counter(df.index_binar)\n",
    "print(indexliste)\n",
    "entfernen('episode_temp_mess.csv')\n",
    "t_corpus = []\n",
    "file = 'episode_binar_mess.csv'\n",
    "with open(file, \"r\", encoding='utf-8') as maerchen:\n",
    "    maerchen = df.episode\n",
    "    reader = csv.reader(maerchen, delimiter = \"|\") \n",
    "    for row in reader:       \n",
    "        lst = str(row)\n",
    "        t_corpus.append(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorhersage von Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 14]\n",
    "\n",
    "1-2: Vorhersagen treffen.\n",
    "\n",
    "3: Der synthetische Datensatz wird ausgeschlossen.\n",
    "\n",
    "4: Jeder positiv bewertete Datensatz wird separat eingeblendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_pred=grid.predict(t_corpus)\n",
    "for index in enumerate(mod_pred): \n",
    "    if '1' in (index) and index[0]>0:   \n",
    "        print ('Positiv bewerteter Datensatz:{}'.format(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 15]\n",
    "\n",
    "1-7: Alle probabilistischen Vorhersagen abrufen. Durch die Anpassung der Variable 'pos>=' in Zeile 6 kann der minimale Score manipuliert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_pred=grid.predict_proba(t_corpus) \n",
    "sort_index=np.flip(all_pred[:,1].argsort())\n",
    "for num_pos in sort_index:\n",
    "    pos=all_pred[num_pos][1]\n",
    "    for i,t_corpus_element in enumerate(t_corpus):\n",
    "        if i==num_pos  and num_pos>0 and pos>0.007:\n",
    "            print(num_pos,'-', pos,'-', df.quelle[num_pos],'-', df.episode[num_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Vorhersage von Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 16]\n",
    "\n",
    "1-14: Daten im Messdatenkorpus Vektorisieren, Transformieren und die Metrik für die Vorhersagefunktion definieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vectorizer = TfidfVectorizer\n",
    "vectorizers = vectorizer(analyzer='word', ngram_range=(1,1),\n",
    "                         min_df = 2, stop_words = de)\n",
    "vokabular =  vectorizers.fit_transform (t_corpus)\n",
    "metrik = cosine_similarity\n",
    "def find_similar(vokabular, index, top_n =-1):   \n",
    "    metriks = metrik(vokabular[index:index+1], vokabular).flatten()\n",
    "    related_docs_indices = [i for i in metriks.argsort()[::-1] \n",
    "                            if i != index]\n",
    "    return [(index, metriks[index]) for index \n",
    "            in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 17]\n",
    "\n",
    "1-3: Gesuchte Episode (Zielepisode) einblenden. Die gesuchte Episode wird durch die 'n_te'-Variable bestimmt. Manuell eingepflegte Zielepisoden sollten deshalb immer in der ersten Reihe der Korpusdatensätze stehen. Wir empfehlen daher, diese Daten mit dem Präfix '0_' zu benennen.\n",
    "\n",
    "4: Synthetische Datei aus dem Messdatenordner entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_te = 0\n",
    "synt = t_corpus[n_te]\n",
    "print(df.quelle[n_te],'-', synt)\n",
    "entfernen('Messdaten/'+'0_'+gesep+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 18]\n",
    "\n",
    "1-3: Suchen starten und die Ergebnisse einblenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, score in (find_similar(vokabular, n_te)):\n",
    "    if (df.index_string[index]==0 and score>0.007):\n",
    "        print(index,'-', score,'-', df.quelle[index],'-', \n",
    "           t_corpus[index], '-',  df.index_binar[index],'-', df.index_binar[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert[webpdf] in c:\\users\\elguj\\appdata\\roaming\\python\\python39\\site-packages (7.12.0)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (4.0.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (2.1.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (0.5.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (21.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (5.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (4.10.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (3.0.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (0.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (1.4.3)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (4.8.1)\n",
      "Requirement already satisfied: nbformat>=5.7 in c:\\users\\elguj\\appdata\\roaming\\python\\python39\\site-packages (from nbconvert[webpdf]) (5.9.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\elguj\\appdata\\roaming\\python\\python39\\site-packages (from nbconvert[webpdf]) (3.0.2)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (2.10.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\elguj\\appdata\\roaming\\python\\python39\\site-packages (from nbconvert[webpdf]) (1.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (4.8.1)\n",
      "Requirement already satisfied: playwright in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbconvert[webpdf]) (1.40.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert[webpdf]) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert[webpdf]) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from importlib-metadata>=3.6->nbconvert[webpdf]) (3.6.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from jupyter-core>=4.7->nbconvert[webpdf]) (228)\n",
      "Requirement already satisfied: async-generator in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbclient>=0.5.0->nbconvert[webpdf]) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbclient>=0.5.0->nbconvert[webpdf]) (1.5.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbclient>=0.5.0->nbconvert[webpdf]) (6.1.12)\n",
      "Requirement already satisfied: tornado>=4.1 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert[webpdf]) (6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert[webpdf]) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert[webpdf]) (22.2.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from nbformat>=5.7->nbconvert[webpdf]) (3.2.0)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\elguj\\appdata\\roaming\\python\\python39\\site-packages (from nbformat>=5.7->nbconvert[webpdf]) (2.19.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert[webpdf]) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert[webpdf]) (21.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert[webpdf]) (58.0.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert[webpdf]) (2.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from packaging->nbconvert[webpdf]) (3.0.4)\n",
      "Requirement already satisfied: greenlet==3.0.1 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from playwright->nbconvert[webpdf]) (3.0.1)\n",
      "Requirement already satisfied: pyee==11.0.1 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from playwright->nbconvert[webpdf]) (11.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from pyee==11.0.1->playwright->nbconvert[webpdf]) (4.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nbconvert[webpdf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\elguj\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from h5py) (1.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions in c:\\users\\elguj\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install typing-extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel in c:\\users\\elguj\\anaconda3\\lib\\site-packages (0.37.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: latex in c:\\users\\elguj\\anaconda3\\lib\\site-packages (0.7.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: shutilwhich in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from latex) (1.1.0)\n",
      "Requirement already satisfied: future in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from latex) (0.18.2)\n",
      "Requirement already satisfied: data in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from latex) (0.4)\n",
      "Requirement already satisfied: tempdir in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from latex) (0.7.1)\n",
      "Requirement already satisfied: six in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from data->latex) (1.16.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from data->latex) (5.1.0)\n",
      "Requirement already satisfied: funcsigs in c:\\users\\elguj\\anaconda3\\lib\\site-packages (from data->latex) (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "pip install latex --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
