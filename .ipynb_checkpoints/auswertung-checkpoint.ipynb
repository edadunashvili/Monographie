{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Arbeitplatz vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/edadunashvili/Monographie.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/Monographie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auswertung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 1]\n",
    "\n",
    "1-2: Die Variable 'repert' dient der gezielten Auswahl von Texten aus den vordefinierten Herkunftsrepertoires im Trainingsdatenrepositorium. Die Variable kann mit einem oder mehreren Werten ausgefüllt werden, beispielsweise ['deu', 'kat'], oder auch leer bleiben, etwa ['']. Die Variable 'typNum' ermöglicht den gezielten Zugriff auf einen bestimmten Typ innerhalb der ausgewählten Repertoires. Auch diese Variable darf leer bleiben.\n",
    "\n",
    "3-7: Bei jeder Aggregation des Forschungsdatenkorpus wird der gleichnamige vorherige Korpus gelöscht.\n",
    "\n",
    "8-60: Die TXT-Dateien im Ordner „Forschungsdaten“ werden im CSV-Format aggregiert und in das Stammverzeichnis verschoben.\n",
    "\n",
    "61-78: Die Datei 'episode_temp_auswerter.csv' wird geladen, von überflüssigen Zeichen bereinigt und in die Korpusdatei 'episode_string_auswerter.csv' umgewandelt.\n",
    "\n",
    "79: Überflüssige Aggregation wird entfernt.\n",
    "\n",
    "80-84: Die im aggregierten Forschungsdatenkorpus vorhandenen Markupelemente (einschließlich der Häufigkeitsindizes) werden abgerufen.\n",
    "\n",
    "Anmerkung: Der Wert 'ballast' bezieht sich auf den ersten Datensatz im aggregierten Datenkorpus. Er stellt sicher, dass die synthetische Episode 'Anf_' auch dann generiert wird, wenn der erste Datensatz im Korpus die gesuchte Episode ist (vgl. Zelle #2, Zeile 32-33)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repert=['kat']\n",
    "typNum=''\n",
    "def entfernen(var):\n",
    "    import os\n",
    "    if os.path.exists(var):\n",
    "        os.remove(var)\n",
    "entfernen('episode_string_auswerter.csv')\n",
    "def word_to_lex(word):\n",
    "    ret=(word) \n",
    "    return ret\n",
    "def write_back(words):\n",
    "    with open('episode_temp_auswerter.csv',\"a\", encoding='utf-8', errors='ignore') as output:\n",
    "        for word in words:\n",
    "            as_lex = word_to_lex(word[0])\n",
    "            full_word = '\"' + as_lex + '\"'\n",
    "            for sub_word in word[1:]:\n",
    "                full_word += \" , \"  '\"' + sub_word + '\"'\n",
    "            full_word +=\"\\n\"\n",
    "            output.write(full_word)\n",
    "def clean(line):\n",
    "    line = line.replace(\"\\n\",\" \").strip()\n",
    "    line = line.replace(\"ä\",\"ae\").replace(\"ü\",\"ue\").replace(\"ö\",\"oe\")\n",
    "    line = line.replace(\"ß\",\"ss\").replace(\",\",\"\").replace(\"«\",\"\")\n",
    "    line = line.replace(\"»\",\"\").replace(\".\",\"\").replace(\":\",\"\")\n",
    "    line = line.replace(\";\",\"\").replace('\"',\"\")\n",
    "    line = line.replace(\"?\",\"\").replace(\"!\",\"\").replace(\"á\",\"a\")\n",
    "    line = line.replace(\",\",\"\").replace(\"\\t\",\" \").replace(\"'\",\"\")\n",
    "    line = line.replace(\"‹\",\"\").replace(\"›\",\"\").replace(\"-\",\" \")\n",
    "    line = line.replace(\"'('\",\"\").replace(\"')'\",\"\").replace('>','')\n",
    "    line = line.replace(\"    \",\" \").replace(\"   \",\" \").replace(\"  \",\" \")\n",
    "    line = line.replace('–','').replace('—','').replace('<','')\n",
    "    line = line.replace(\"Â\", \"A\").replace(\"ø\", \"oe\").replace('“','')\n",
    "    line = line.replace('„','').replace('(','').replace(')','')\n",
    "    line = line.replace(\"‚\", \"\").replace(']','').replace('[','')\n",
    "    if line == \"\": \n",
    "        return\n",
    "    line=line.split(\"|\")\n",
    "    line[0]=line[0].split(\"|\")[0]\n",
    "    for i, _ in enumerate(line):\n",
    "        if (i !=0) and (i!=2):\n",
    "            line[i]=line[i].lower()\n",
    "    flex=[]\n",
    "    try:\n",
    "        flex=line[1].split(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    value=str(line)\n",
    "    line=str(line)   \n",
    "    flex.append(line)\n",
    "    ret=[]\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "    for i in flex:\n",
    "        ret.append((i,value[0]))\n",
    "    return ret\n",
    "with open('episode_temp_auswerter.csv', \"w\", encoding='utf-8', errors='ignore') as output:\n",
    "    output.write (\"quelle,episode,index_string,index_binar\\n\")\n",
    "    output.write('000_ballast,ballast,ballast,0')\n",
    "    output.write('\\n')\n",
    "pairs = []\n",
    "import glob as glob\n",
    "for file in glob.glob(\"Forschungsdaten/*.txt\"):  \n",
    "    for any in repert:\n",
    "        if any in file and typNum in file:\n",
    "            with open(file, 'r', encoding='utf-8', errors='ignore') as episode:  \n",
    "                for line in episode.readlines():\n",
    "                        clean_words = clean(line)\n",
    "                        pairs = pairs + clean_words\n",
    "write_back(pairs)\n",
    "fin = open('episode_temp_auswerter.csv','r', encoding ='utf-8')\n",
    "fout = open('episode_string_auswerter.csv', \"wt\", encoding ='utf-8')\n",
    "for kfz in fin:\n",
    "    fout.write(kfz.replace(', \"[\"',\"\").replace('\"[', \"\")\n",
    "               .replace(']\"',\"\").replace(\"', '\", \"','\")\n",
    "               .replace(\" '\", \"'\").replace(\"'\",\"\").replace ('\"',''))\n",
    "fin.close()\n",
    "fout.close()\n",
    "entfernen('episode_temp_auswerter.csv')\n",
    "import pandas as pd\n",
    "df = pd.read_csv('episode_string_auswerter.csv', encoding='utf-8')\n",
    "from collections import Counter\n",
    "indexliste=Counter(df.index_string)\n",
    "print(indexliste, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 2] \n",
    "\n",
    "1-40: Das Skript sorgt dafür, dass die in den betrachteten Texten vorhandenen Annotationen in eine Kette der vorherigen, der gesuchten und der folgenden Episoden als Gliedern der gesamten Kette dargestellt werden.\n",
    "\n",
    "41-50: Das Skript extrachiert alle im Korpus vorhandene Episoden.\n",
    "\n",
    "51-66: Die extrahierte Episoden werden an die Glieder der gesuchten Episodengruppe umlegen. Die festlegung der gesuchten Episodengruppe sowie Erschienen der mit einander verkettete Episodenliste erfolgt in der nächsten Zelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('episode_string_auswerter.csv', encoding='utf-8')\n",
    "from collections import Counter\n",
    "def ep_format(ep_full):\n",
    "    return (ep_full.split('_'))[0]\n",
    "def ep_name_format(ep_full):\n",
    "    sublist = (ep_full.split('_'))[0:2]\n",
    "    name = \"\"\n",
    "    for strg in sublist:\n",
    "        name += strg+'_'\n",
    "    return name\n",
    "def ep_name_vollformat(ep_full):\n",
    "    sublist = (ep_full.split())\n",
    "    vollname = \"\"\n",
    "    for strg in sublist:\n",
    "        vollname += strg\n",
    "    return vollname\n",
    "def quellenvergleich (df, i1, i2):\n",
    "    return df.quelle[i1]==df.quelle[i2]\n",
    "def ast(gesep, df):\n",
    "    ep_tree = {}\n",
    "    a_liste = []\n",
    "    z_liste = []  \n",
    "    df_len = len(df.index_string)\n",
    "    for i, ep in enumerate(df.index_string):\n",
    "        if gesep == ep:\n",
    "            if (i > 0)&(quellenvergleich(df, i, i-1)):\n",
    "                a = df.index_string[i-1]\n",
    "            else:\n",
    "                a = 'Anf-'+ep_name_format(gesep)\n",
    "            if (i < df_len - 1):\n",
    "                if not (quellenvergleich(df, i, i+1)):\n",
    "                    z = 'End-'+ep_name_format(gesep)\n",
    "                else:        \n",
    "                    z = df.index_string[i+1]\n",
    "            else:\n",
    "                z = 'End-'+ep_name_format(gesep)\n",
    "            a_liste.append(a)\n",
    "            z_liste.append(z)\n",
    "    return {gesep: [Counter(a_liste), Counter(z_liste)]}\n",
    "def alle_aeste(gesep, df):\n",
    "    episoden_baeume = {} \n",
    "    ep_list = []\n",
    "    for ep_full in df.index_string:         \n",
    "        ep = ep_format(ep_full)      \n",
    "        if gesep == ep:                      \n",
    "            ep_list.append(ep_full)\n",
    "    for ep in set(ep_list):\n",
    "        episoden_baeume.update(ast(ep,df))\n",
    "    return episoden_baeume\n",
    "def key_val_printer(d):\n",
    "    for k, v in d.items():\n",
    "        print(v, k, sep=':')\n",
    "def neukey_val_printer(d):\n",
    "    for k, v in d.items():\n",
    "        print(k)\n",
    "def baum_printer(baeume: dict, baum: str):\n",
    "    key_val_printer (baeume[baum][0])\n",
    "    print(65*'-')\n",
    "    print(sum(baeume[baum][0].values()),':', baum)\n",
    "    print(65*'-')\n",
    "    key_val_printer(baeume[baum][1])\n",
    "def wald_printer(wald: dict):\n",
    "    for baum in sorted(wald.keys()): # for baum in (wald.keys()):\n",
    "        baum_printer (wald, baum)\n",
    "        print(65*'=')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 3] - Hier wird die gesuchte Episode allein oder mit den Episodengruppen aus dem kombinierten Typ bestimmt. Nach Ausführung des Skripts wird die entsprechende Episodenkette ausgewertet. Die resultierende Liste besteht aus Tripletten (durch doppelte Linien getrennte Zeilengruppen). Jede Triplette setzt sich aus einer mittleren Episode (von einfachen Linien umgebene Zeilen) zusammen, die wiederum von einer oder mehreren oberen und unteren Episoden umgeben ist. Jedes Glied aus der gesuchten Episodengruppe soll als Triplette aller drei Mitglieder dargestellt werden. Falls eine Episode keine Anfangs- oder Ergänzungsglieder hat, werden an den entsprechenden Stellen der Liste synthetische Episoden generiert, wie 'Anf-e123_a_' oder 'End-e123_a_'. Somit stellt jede Episode des gesuchten Typs eine mittlere Episode dar und tritt in Bezug auf andere Episoden (der gleichen oder anderer Kategorien) als vorläufige und/oder nachfolgende Episode auf.\n",
    "\n",
    "1: Eingabefeld für die gesuchte Episode\n",
    "\n",
    "2 - n: Eingabefeld für kombinierte Episodengruppen (das Eingabefeld <graph.update(alle_aeste(\"'\", df)> kann kopiert und vervielfacht werden.\n",
    "\n",
    "Letzte Zeile: Visualisierung der Episodenkette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = alle_aeste(\"e301\", df)\n",
    "graph.update(alle_aeste(\"e650A\", df))\n",
    "graph.update(alle_aeste(\"e303\", df))\n",
    "graph.update(alle_aeste(\"e300\", df))\n",
    "graph.update(alle_aeste(\"e516\", df))\n",
    "graph.update(alle_aeste(\"\", df))\n",
    "wald_printer(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 4]\n",
    "\n",
    "1-78: Die in der obigen Liste dargestellten Begegnungen von Typen bzw. Episoden werden in Graphen umgewandelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "def cutoff_ast_data(wuerzel, graph, nachbar_liste, cutoff = 0):\n",
    "    vor = graph[wuerzel][0]\n",
    "    zurueck = graph[wuerzel][1]\n",
    "    nachbarn = vor.copy()\n",
    "    nachbarn.update(zurueck)\n",
    "    for ast_name, ast_gewicht in sorted(nachbarn.items()):\n",
    "        if (ast_gewicht >= cutoff):\n",
    "             nachbar_liste.append([ep_name_format(wuerzel), \n",
    "                                   ep_name_format(ast_name), ast_gewicht]) \n",
    "def cutoff_ast_volldata(wuerzel, graph, nachbar_liste, cutoff = 0):\n",
    "    vor = graph[wuerzel][0]\n",
    "    zurueck = graph[wuerzel][1]\n",
    "    nachbarn = vor.copy()\n",
    "    nachbarn.update(zurueck)\n",
    "    for ast_name, ast_gewicht in sorted(nachbarn.items()):\n",
    "        if (ast_gewicht >= cutoff):\n",
    "             #nachbar_liste.append([ep_name_vollformat(wuerzel), ep_name_vollformat(ast_name), ast_gewicht])\n",
    "            nachbar_liste.append([wuerzel, ast_name, ast_gewicht]) \n",
    "def cutoff_graph_data(graph, cutoff = 0):\n",
    "    nachbar_liste = []\n",
    "    for i, (k, v) in enumerate(sorted(graph.items())):\n",
    "        cutoff_ast_data(k, graph, nachbar_liste, cutoff)\n",
    "    neue_nachbar_liste = [[i, nachbar] for i, nachbar in enumerate(nachbar_liste)]\n",
    "    return neue_nachbar_liste\n",
    "def cutoff_graph_volldata(graph, cutoff = 0):\n",
    "    nachbar_liste = []\n",
    "    for i, (k, v) in enumerate(sorted(graph.items())):\n",
    "        cutoff_ast_volldata(k, graph, nachbar_liste, cutoff)\n",
    "    neue_nachbar_liste = [[i, nachbar] for i, nachbar in enumerate(nachbar_liste)]\n",
    "    return neue_nachbar_liste\n",
    "def interactive_graph_data(graph_data, loesch_index, gew_dict):\n",
    "    for k, v in gew_dict.items():\n",
    "        graph_data[k][1][2] = v\n",
    "    neue_nachbar_liste = [[i, nachbar[1]] for i, \n",
    "                          nachbar in enumerate(graph_data) if i not in loesch_index]\n",
    "    return neue_nachbar_liste\n",
    "def graph_bauer(graph_data):\n",
    "    G = nx.Graph()\n",
    "    w_liste=[]\n",
    "    for el in graph_data:\n",
    "        n1 = el[1][0]\n",
    "        n2 = el[1][1]\n",
    "        w = el[1][2]\n",
    "        w_liste.append(w)\n",
    "        G.add_edge(n1, n2, weight=w)\n",
    "    return G,w_liste\n",
    "def show_graph(G,w):\n",
    "    fh = open(\"edgelist.utf-8\", \"wb\")\n",
    "    nx.write_multiline_adjlist(G, fh, \n",
    "                               delimiter=\"\\t\", encoding=\"utf-8\")\n",
    "    fh = open(\"edgelist.utf-8\", \"rb\")\n",
    "    H = nx.read_multiline_adjlist(fh, \n",
    "                                  delimiter=\"\\t\", encoding=\"utf-8\")\n",
    "    for n in G.nodes():\n",
    "        if n not in H:\n",
    "            print(False)\n",
    "    pos = nx.spring_layout(G)\n",
    "    q = max(w)\n",
    "    klein = q //3\n",
    "    gross = q //2  \n",
    "    wenig = [(u, v) for (u, v, d) in G.edges(data=True) \n",
    "             if d[\"weight\"] <klein]\n",
    "    mehr = [(u, v) for (u, v, d) in G.edges(data=True) \n",
    "            if klein <= d[\"weight\"] <=gross]\n",
    "    viel = [(u, v) for (u, v, d) in G.edges(data=True) \n",
    "            if d[\"weight\"] >gross]\n",
    "    nx.draw(G, pos, edgelist=wenig, font_size=8, \n",
    "            edge_color=\"g\", width=1, with_labels=False)\n",
    "    nx.draw(G, pos, edgelist=mehr, font_size=8, \n",
    "            edge_color=\"b\", width=1, with_labels=False)\n",
    "    nx.draw(G, pos, edgelist=viel, font_size=8, \n",
    "            edge_color=\"r\", width=1, with_labels=False)\n",
    "    for p in pos:  \n",
    "        pos[p][0] -=0.00\n",
    "    nx.draw_networkx_labels(G, pos, font_size=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Zelle # 5] -  Visualisierung der Normalform des Typs durch die Graphen.\n",
    "1: Häufigkeitsindex anpassen.\n",
    "\n",
    "2-3: Der Inhalt wird definiert.\n",
    "\n",
    "4-5: Format (Höhe und Breite) bestimmen und visualisieren lassen (kann manipuliert werden).\n",
    "\n",
    "Die Farben der Kanten haben folgende Bedeutung:\n",
    "\n",
    "• grün = selten (<=33%)\n",
    "\n",
    "• blau = mittel (>33% & <50%)\n",
    "\n",
    "• rot = häufig (>= 50%)\n",
    "\n",
    "6-18: Vollständige Namen der in den visualisierten Graphen anzutreffenden Knoten (in alphabetischer Reihenfolge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "haeufigkeit=2\n",
    "auto_graph=cutoff_graph_data(graph, haeufigkeit)\n",
    "G,W = graph_bauer(auto_graph)\n",
    "plt.figure(figsize=(12, 12)) #15,12 || 12,9 || 9,9\n",
    "show_graph(G,W)\n",
    "def make_set_of_chosen_vertives(graph_edge_data):\n",
    "    chosen_vertices=set()\n",
    "    for sub_list in graph_edge_data:\n",
    "        sub_sub_list = sub_list[1]\n",
    "        chosen_vertices.add(sub_sub_list[0])\n",
    "        chosen_vertices.add(sub_sub_list[1])\n",
    "    return chosen_vertices\n",
    "graph_data = cutoff_graph_volldata(graph, haeufigkeit)\n",
    "chosen_vertices=list(make_set_of_chosen_vertives(graph_data))\n",
    "chosen_vertices.sort()    \n",
    "for vertex_name in chosen_vertices:\n",
    "    if not (vertex_name.startswith(\"End\") or vertex_name.startswith(\"Anf\")or vertex_name.startswith(\"eUNDF\")):\n",
    "        print(vertex_name)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
